{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeenShaikh/AskPy/blob/main/AskPy_Agentic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ![icon (2).png](data:image/png;base64,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)**AskPy**\n"
      ],
      "metadata": {
        "id": "z2NwlTluRNJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  streamlit \\\n",
        "  sentence-transformers \\\n",
        "  faiss-cpu \\\n",
        "  transformers \\\n",
        "  langfuse \\\n",
        "  pyngrok \\\n",
        "  fastapi \\\n",
        "  uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H3KfDYomTRu",
        "outputId": "a7a3e8e4-22f1-4fd9-d875-e5ca62b17361"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opentelemetry-api==1.37.0 opentelemetry-sdk==1.37.0 -q\n",
        "!pip install fastmcp --no-deps -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh8nI6Amsd6m",
        "outputId": "8a46886a-4805-4084-ed8d-7e3a03b69ccf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/65.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/131.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/208.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-prometheus 0.60b1 requires opentelemetry-sdk~=1.39.1, but you have opentelemetry-sdk 1.37.0 which is incompatible.\n",
            "opentelemetry-instrumentation 0.60b1 requires opentelemetry-semantic-conventions==0.60b1, but you have opentelemetry-semantic-conventions 0.58b0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-genai -q"
      ],
      "metadata": {
        "id": "MMUYlqIdyQmx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up ngrok and Langfuse keys securely\n",
        "- `ngrok token`: Create a public link for the Streamlit app.\n",
        "- `Langfuse API keys`: Connect securely to Langfuse for observability."
      ],
      "metadata": {
        "id": "W_kCSx_YOr8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "#token = getpass(\"Enter your ngrok token: \")\n",
        "#ngrok.set_auth_token(token)\n",
        "secret_key = getpass(\"Enter your Langfuse Secret Key: \")\n",
        "public_key = getpass(\"Enter your Langfuse Public Key: \")\n",
        "\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = secret_key\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = public_key\n",
        "os.environ[\"LANGFUSE_BASE_URL\"] = \"https://cloud.langfuse.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxp0G-aDbSV7",
        "outputId": "6c97b0b5-f9d8-4694-b303-bf94ee66acb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Langfuse Secret Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your Langfuse Public Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Gemini API Key from AI Studio: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-X4O3bqdHd",
        "outputId": "fd8cdaf4-8418-4ba6-d4e7-7bbf704f6a40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Gemini API Key from AI Studio: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Data from Google Drive\n",
        "- `data_py.json` : The documents used for RAG retrieval.\n",
        "- `icon.png` : Application icon.\n",
        "- `trans.png` : Transparent image used as chat avatar placeholder."
      ],
      "metadata": {
        "id": "DX5DXMg7O4JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/AskPy2/data_py.json\"\n",
        "icon_path = \"/content/drive/MyDrive/AskPy2/icon.png\"\n",
        "trans_path = \"/content/drive/MyDrive/AskPy2/trans.png\"\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Resize the transparent image to 1x1 pixels to ensure it functions as just empty space.\n",
        "\n",
        "img = Image.open(trans_path)\n",
        "img = img.resize((1, 1), Image.Resampling.LANCZOS)\n",
        "img.save(trans_path)\n",
        "\n",
        "# Verify the number of documents uploaded\n",
        "\n",
        "print(f\"Loaded {len(dataset)} documents\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts-1QCpUelL3",
        "outputId": "35dcf71f-d220-44b7-aab4-d0b0f172810c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded 245 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_tool.py\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import os\n",
        "import google.genai as genai\n",
        "\n",
        "\n",
        "# ---------------------- Load Docs ----------------------\n",
        "def load_docs(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = json.load(f)\n",
        "    docs = []\n",
        "    for d in raw:\n",
        "        title = d.get(\"title\", \"\")\n",
        "        content = d.get(\"content\", \"\")\n",
        "        examples = d.get(\"example\", \"\")\n",
        "        if isinstance(examples, list):\n",
        "            examples_text = \"\\n\".join(examples)\n",
        "        elif isinstance(examples, str):\n",
        "            examples_text = examples\n",
        "        else:\n",
        "            examples_text = \"\"\n",
        "        full = f\"{title}\\n\\n{content}\"\n",
        "        if examples_text:\n",
        "            full += f\"\\n\\nExamples:\\n{examples_text}\"\n",
        "        docs.append(full)\n",
        "    return docs\n",
        "\n",
        "# ---------------------- Embeddings + Index ----------------------\n",
        "def load_embedder():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def build_index(docs, embedder):\n",
        "    vecs = embedder.encode(docs, convert_to_numpy=True)\n",
        "    vecs = vecs / np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    index.add(vecs)\n",
        "    return vecs, index\n",
        "\n",
        "def search_docs(query, embedder, vectors, index, k=2):\n",
        "    qvec = embedder.encode([query], convert_to_numpy=True)\n",
        "    qvec = qvec / np.linalg.norm(qvec)\n",
        "    _, ids = index.search(qvec.reshape(1, -1), k)\n",
        "    return ids[0]\n",
        "\n",
        "# ---------------------- LLM using Gemini ----------------------\n",
        "def load_llm():\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"Gemini API key not found in environment variable GOOGLE_API_KEY\")\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    def pipe(prompt):\n",
        "        response = client.models.generate_content(\n",
        "            model=\"models/gemini-2.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        return [{\"generated_text\": response.text}]\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def generate_answer(context, user_question, pipe):\n",
        "    prompt = f\"\"\"\n",
        "Use the context below to answer the question.\n",
        "If the answer is contained in the context, use only that information.\n",
        "If the answer is not in the context, generate a clear, concise answer in 1-3 sentences, with at most one example if helpful.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{user_question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    return pipe(prompt)[0][\"generated_text\"].strip()\n",
        "\n",
        "\n",
        "\n",
        "def askpy_rag_tool(question: str) -> str:\n",
        "    file_path = \"/content/drive/MyDrive/AskPy2/data_py.json\"\n",
        "    docs = load_docs(file_path)\n",
        "    embedder = load_embedder()\n",
        "    vectors, index = build_index(docs, embedder)\n",
        "    pipe = load_llm()\n",
        "\n",
        "    ids = search_docs(question, embedder, vectors, index, k=2)\n",
        "    context = \"\\n\\n---\\n\\n\".join(docs[i] for i in ids)\n",
        "\n",
        "    answer = generate_answer(context, question, pipe)\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n4LFnZcmOQA",
        "outputId": "f58e7960-9635-477f-a16a-285f96733a44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_tool.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agent_adk.py\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.tools import FunctionTool\n",
        "from rag_tool import askpy_rag_tool\n",
        "\n",
        "answer_tool = FunctionTool(func=askpy_rag_tool)\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"AskPyAgent\",\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    instruction=\"You answer user questions using the RAG tool\",\n",
        "    tools=[answer_tool]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    question = input(\"write your question: \")\n",
        "    answer = askpy_rag_tool(question)\n",
        "    print(\"answer\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQyrBGOymPKK",
        "outputId": "1c3f519d-10dc-4d43-9aa5-2f4c7412abf6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting agent_adk.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mcp_server.py\n",
        "\n",
        "from fastmcp import FastMCP\n",
        "from rag_tool import askpy_rag_tool\n",
        "\n",
        "mcp = FastMCP(\"AskPy MCP Server\")\n",
        "\n",
        "@mcp.tool(\n",
        "    name=\"ask_python\",\n",
        "    description=\"Answer Python questions using a RAG-based knowledge base\"\n",
        ")\n",
        "def ask_python(question: str) -> str:\n",
        "    \"\"\"\n",
        "    This tool takes a Python-related question and returns an answer\n",
        "    using Retrieval-Augmented Generation (RAG).\n",
        "    \"\"\"\n",
        "    return askpy_rag_tool(question)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzKRAeDNtGGn",
        "outputId": "e74b51c1-bc86-4124-d3a9-8ab08aa11477"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mcp_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "from langfuse import Langfuse\n",
        "from rag_tool import askpy_rag_tool\n",
        "\n",
        "# ---------------------- Langfuse ----------------------\n",
        "langfuse = Langfuse(\n",
        "    public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"],\n",
        "    secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"],\n",
        "    host=os.environ[\"LANGFUSE_BASE_URL\"]\n",
        ")\n",
        "\n",
        "# ---------------------- Paths ----------------------\n",
        "icon_path = \"/content/drive/MyDrive/AskPy2/icon.png\"\n",
        "trans_path = \"/content/drive/MyDrive/AskPy2/trans.png\"\n",
        "\n",
        "# ---------------------- Streamlit Page Config ----------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"AskPy - Agentic Python Tutor\",\n",
        "    layout=\"centered\",\n",
        "    page_icon=icon_path\n",
        ")\n",
        "\n",
        "# ---------------------- Header ----------------------\n",
        "col1, col2 = st.columns([1, 8])\n",
        "with col1:\n",
        "    st.image(icon_path, width=100)\n",
        "with col2:\n",
        "    st.markdown(\"# AskPy â€” Agentic Python AI Tutor\")\n",
        "    st.caption(\"RAG-powered â€¢ Gemini â€¢ MCP-ready\")\n",
        "\n",
        "# ---------------------- Chat State ----------------------\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display previous messages\n",
        "for role, msg in st.session_state.messages:\n",
        "    if role == \"user\":\n",
        "        st.chat_message(\"user\", avatar=trans_path).markdown(msg)\n",
        "    else:\n",
        "        st.chat_message(\"assistant\", avatar=icon_path).markdown(msg)\n",
        "\n",
        "# ---------------------- Chat Input ----------------------\n",
        "query = st.chat_input(\"Ask a Python question...\")\n",
        "\n",
        "if query:\n",
        "    # show user message\n",
        "    st.session_state.messages.append((\"user\", query))\n",
        "    st.chat_message(\"user\", avatar=trans_path).markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\", avatar=icon_path):\n",
        "        placeholder = st.empty()\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "\n",
        "            # Root observation\n",
        "            with langfuse.start_as_current_observation(\n",
        "                as_type=\"span\",\n",
        "                name=\"AskPy User Question\"\n",
        "            ):\n",
        "\n",
        "                # Retrieval + Generation combined inside RAG tool\n",
        "                with langfuse.start_as_current_observation(\n",
        "                    as_type=\"generation\",\n",
        "                    name=\"RAG + Gemini Generation\"\n",
        "                ) as span:\n",
        "\n",
        "                    answer = askpy_rag_tool(query)\n",
        "\n",
        "                    span.update(\n",
        "                        input=query,\n",
        "                        output=answer,\n",
        "                        metadata={\n",
        "                            \"model\": \"gemini-2.5-flash\",\n",
        "                            \"pipeline\": \"RAG\",\n",
        "                            \"agentic\": True\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            placeholder.markdown(answer)\n",
        "\n",
        "    st.session_state.messages.append((\"assistant\", answer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAB5xmLFS9-H",
        "outputId": "cf3c959d-6dc8-497a-8e87-f97715ae1631"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python mcp_server.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWuj3R1stQyS",
        "outputId": "1dc0ffcc-56a6-411a-bc8c-3a34baed4656"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 14:26:25.421639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766586385.443984    7939 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766586385.450527    7939 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766586385.467806    7939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766586385.467850    7939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766586385.467854    7939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766586385.467859    7939 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 14:26:25.472936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\n",
            "\n",
            "\u001b[2mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                                                                              \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                        \u001b[36m \u001b[0m\u001b[36mâ–„\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–„\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–„\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[94m \u001b[0m\u001b[94mâ–ˆ\u001b[0m\u001b[94mâ–€\u001b[0m\u001b[94mâ–€\u001b[0m\u001b[94m \u001b[0m\u001b[94mâ–ˆ\u001b[0m\u001b[94mâ–€\u001b[0m\u001b[94mâ–ˆ\u001b[0m                        \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                        \u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36m \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–„\u001b[0m\u001b[36mâ–„\u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36m \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36m \u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–€\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ–ˆ\u001b[0m\u001b[94m \u001b[0m\u001b[94mâ–ˆ\u001b[0m\u001b[94mâ–„\u001b[0m\u001b[94mâ–„\u001b[0m\u001b[94m \u001b[0m\u001b[94mâ–ˆ\u001b[0m\u001b[94mâ–€\u001b[0m\u001b[94mâ–€\u001b[0m                        \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                                                                              \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                                \u001b[1;34mFastMCP 2.14.1\u001b[0m                                \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                                                                              \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                                                                              \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                    \u001b[1mðŸ–¥ \u001b[0m\u001b[1m \u001b[0m\u001b[36mServer name:\u001b[0m\u001b[36m \u001b[0m\u001b[1;2;34mAskPy MCP Server     \u001b[0m                     \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                    \u001b[1m   \u001b[0m\u001b[36m             \u001b[0m\u001b[1;2;34m                     \u001b[0m                     \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                    \u001b[1mðŸ“¦\u001b[0m\u001b[1m \u001b[0m\u001b[36mTransport:  \u001b[0m\u001b[36m \u001b[0m\u001b[2mSTDIO                \u001b[0m                     \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[36m            \u001b[0m\u001b[36m \u001b[0m\u001b[2m                     \u001b[0m                     \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                    \u001b[1mðŸ“š\u001b[0m\u001b[1m \u001b[0m\u001b[36mDocs:       \u001b[0m\u001b[36m \u001b[0m\u001b[2mhttps://gofastmcp.com\u001b[0m                     \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                    \u001b[1mðŸš€\u001b[0m\u001b[1m \u001b[0m\u001b[36mHosting:    \u001b[0m\u001b[36m \u001b[0m\u001b[2mhttps://fastmcp.cloud\u001b[0m                     \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ”‚\u001b[0m                                                                              \u001b[2mâ”‚\u001b[0m\n",
            "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[2;36m[12/24/25 14:26:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'AskPy MCP \u001b[0m      \u001b]8;id=67631;file:///usr/local/lib/python3.12/dist-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=592130;file:///usr/local/lib/python3.12/dist-packages/fastmcp/server/server.py#2527\u001b\\\u001b[2m2527\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m                    \u001b[0m         \u001b[32mServer'\u001b[0m with transport \u001b[32m'stdio'\u001b[0m       \u001b[2m              \u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .gitignore\n",
        "*.gdoc\n",
        "*.gsheet\n",
        "*.gslides\n",
        "*.ipynb_checkpoints/\n",
        "drive/\n",
        "__pycache__/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRhD1_aNzfpP",
        "outputId": "45a7bf6f-5625-4375-b613-2e5df53c0b49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .gitignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FwrWNkdwJtd",
        "outputId": "3038de35-f2da-4ecd-86f1-b26dca71c3cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\n",
            "Aborted!\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.12/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1575, in _shutdown\n",
            "    def _shutdown():\n",
            "    \n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running and Connecting the Streamlit App to ngrok\n",
        "- Run Streamlit app on port 8501\n",
        "- Restart ngrok to avoid pending sessions\n",
        "- Generate a public URL to access the app from outside Colab"
      ],
      "metadata": {
        "id": "3_nNoS_ZQ1vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 > /content/log.txt 2>&1 &\n",
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB0Da_m-TVce",
        "outputId": "b097d0b8-d124-4a84-b84f-22cc2a92d8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://293c211ecdad.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}