# -*- coding: utf-8 -*-
"""AskPy-Agentic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q9WTR_MonLEUQDEGhNI1UEsU9ezW2rVs

# ![icon (2).png](data:image/png;base64,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)**AskPy**
"""

!pip install -q \
  streamlit \
  sentence-transformers \
  faiss-cpu \
  transformers \
  langfuse \
  pyngrok \
  fastapi \
  uvicorn

!pip install opentelemetry-api==1.37.0 opentelemetry-sdk==1.37.0 -q
!pip install fastmcp --no-deps -q

!pip install --upgrade google-genai -q

"""## Setting up ngrok and Langfuse keys securely
- `ngrok token`: Create a public link for the Streamlit app.
- `Langfuse API keys`: Connect securely to Langfuse for observability.
"""

from getpass import getpass
import os

#token = getpass("Enter your ngrok token: ")
#ngrok.set_auth_token(token)
secret_key = getpass("Enter your Langfuse Secret Key: ")
public_key = getpass("Enter your Langfuse Public Key: ")

os.environ["LANGFUSE_SECRET_KEY"] = secret_key
os.environ["LANGFUSE_PUBLIC_KEY"] = public_key
os.environ["LANGFUSE_BASE_URL"] = "https://cloud.langfuse.com"

from getpass import getpass
import os

# Gemini API key
os.environ["GOOGLE_API_KEY"] = getpass("Enter your Gemini API Key from AI Studio: ")

"""## Uploading Data from Google Drive
- `data_py.json` : The documents used for RAG retrieval.
- `icon.png` : Application icon.
- `trans.png` : Transparent image used as chat avatar placeholder.
"""

from google.colab import drive
import json
import os
from PIL import Image

drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/AskPy2/data_py.json"
icon_path = "/content/drive/MyDrive/AskPy2/icon.png"
trans_path = "/content/drive/MyDrive/AskPy2/trans.png"

with open(file_path, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# Resize the transparent image to 1x1 pixels to ensure it functions as just empty space.

img = Image.open(trans_path)
img = img.resize((1, 1), Image.Resampling.LANCZOS)
img.save(trans_path)

# Verify the number of documents uploaded

print(f"Loaded {len(dataset)} documents")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile rag_tool.py
# 
# import json
# import numpy as np
# from sentence_transformers import SentenceTransformer
# import faiss
# import os
# import google.genai as genai
# 
# 
# # ---------------------- Load Docs ----------------------
# def load_docs(file_path):
#     with open(file_path, "r", encoding="utf-8") as f:
#         raw = json.load(f)
#     docs = []
#     for d in raw:
#         title = d.get("title", "")
#         content = d.get("content", "")
#         examples = d.get("example", "")
#         if isinstance(examples, list):
#             examples_text = "\n".join(examples)
#         elif isinstance(examples, str):
#             examples_text = examples
#         else:
#             examples_text = ""
#         full = f"{title}\n\n{content}"
#         if examples_text:
#             full += f"\n\nExamples:\n{examples_text}"
#         docs.append(full)
#     return docs
# 
# # ---------------------- Embeddings + Index ----------------------
# def load_embedder():
#     return SentenceTransformer("all-MiniLM-L6-v2")
# 
# def build_index(docs, embedder):
#     vecs = embedder.encode(docs, convert_to_numpy=True)
#     vecs = vecs / np.linalg.norm(vecs, axis=1, keepdims=True)
#     index = faiss.IndexFlatIP(vecs.shape[1])
#     index.add(vecs)
#     return vecs, index
# 
# def search_docs(query, embedder, vectors, index, k=2):
#     qvec = embedder.encode([query], convert_to_numpy=True)
#     qvec = qvec / np.linalg.norm(qvec)
#     _, ids = index.search(qvec.reshape(1, -1), k)
#     return ids[0]
# 
# # ---------------------- LLM using Gemini ----------------------
# def load_llm():
#     api_key = os.environ.get("GOOGLE_API_KEY")
#     if not api_key:
#         raise ValueError("Gemini API key not found in environment variable GOOGLE_API_KEY")
#     client = genai.Client(api_key=api_key)
# 
#     def pipe(prompt):
#         response = client.models.generate_content(
#             model="models/gemini-2.5-flash",
#             contents=prompt
#         )
#         return [{"generated_text": response.text}]
# 
#     return pipe
# 
# def generate_answer(context, user_question, pipe):
#     prompt = f"""
# Use the context below to answer the question.
# If the answer is contained in the context, use only that information.
# If the answer is not in the context, generate a clear, concise answer in 1-3 sentences, with at most one example if helpful.
# 
# Context:
# {context}
# 
# Question:
# {user_question}
# 
# Answer:
# """
#     return pipe(prompt)[0]["generated_text"].strip()
# 
# 
# 
# def askpy_rag_tool(question: str) -> str:
#     file_path = "/content/drive/MyDrive/AskPy2/data_py.json"
#     docs = load_docs(file_path)
#     embedder = load_embedder()
#     vectors, index = build_index(docs, embedder)
#     pipe = load_llm()
# 
#     ids = search_docs(question, embedder, vectors, index, k=2)
#     context = "\n\n---\n\n".join(docs[i] for i in ids)
# 
#     answer = generate_answer(context, question, pipe)
# 
#     return answer
# 
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile agent_adk.py
# from google.adk.agents import Agent
# from google.adk.tools import FunctionTool
# from rag_tool import askpy_rag_tool
# 
# answer_tool = FunctionTool(func=askpy_rag_tool)
# 
# agent = Agent(
#     name="AskPyAgent",
#     model="gemini-2.5-flash",
#     instruction="You answer user questions using the RAG tool",
#     tools=[answer_tool]
# )
# 
# if __name__ == "__main__":
#     question = input("write your question: ")
#     answer = askpy_rag_tool(question)
#     print("answer", answer)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile mcp_server.py
# 
# from fastmcp import FastMCP
# from rag_tool import askpy_rag_tool
# 
# mcp = FastMCP("AskPy MCP Server")
# 
# @mcp.tool(
#     name="ask_python",
#     description="Answer Python questions using a RAG-based knowledge base"
# )
# def ask_python(question: str) -> str:
#     """
#     This tool takes a Python-related question and returns an answer
#     using Retrieval-Augmented Generation (RAG).
#     """
#     return askpy_rag_tool(question)
# 
# 
# if __name__ == "__main__":
#     mcp.run()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import os
# from langfuse import Langfuse
# from rag_tool import askpy_rag_tool
# 
# # ---------------------- Langfuse ----------------------
# langfuse = Langfuse(
#     public_key=os.environ["LANGFUSE_PUBLIC_KEY"],
#     secret_key=os.environ["LANGFUSE_SECRET_KEY"],
#     host=os.environ["LANGFUSE_BASE_URL"]
# )
# 
# # ---------------------- Paths ----------------------
# icon_path = "/content/drive/MyDrive/AskPy2/icon.png"
# trans_path = "/content/drive/MyDrive/AskPy2/trans.png"
# 
# # ---------------------- Streamlit Page Config ----------------------
# st.set_page_config(
#     page_title="AskPy - Agentic Python Tutor",
#     layout="centered",
#     page_icon=icon_path
# )
# 
# # ---------------------- Header ----------------------
# col1, col2 = st.columns([1, 8])
# with col1:
#     st.image(icon_path, width=100)
# with col2:
#     st.markdown("# AskPy — Agentic Python AI Tutor")
#     st.caption("RAG-powered • Gemini • MCP-ready")
# 
# # ---------------------- Chat State ----------------------
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# 
# # Display previous messages
# for role, msg in st.session_state.messages:
#     if role == "user":
#         st.chat_message("user", avatar=trans_path).markdown(msg)
#     else:
#         st.chat_message("assistant", avatar=icon_path).markdown(msg)
# 
# # ---------------------- Chat Input ----------------------
# query = st.chat_input("Ask a Python question...")
# 
# if query:
#     # show user message
#     st.session_state.messages.append(("user", query))
#     st.chat_message("user", avatar=trans_path).markdown(query)
# 
#     with st.chat_message("assistant", avatar=icon_path):
#         placeholder = st.empty()
#         with st.spinner("Thinking..."):
# 
#             # Root observation
#             with langfuse.start_as_current_observation(
#                 as_type="span",
#                 name="AskPy User Question"
#             ):
# 
#                 # Retrieval + Generation combined inside RAG tool
#                 with langfuse.start_as_current_observation(
#                     as_type="generation",
#                     name="RAG + Gemini Generation"
#                 ) as span:
# 
#                     answer = askpy_rag_tool(query)
# 
#                     span.update(
#                         input=query,
#                         output=answer,
#                         metadata={
#                             "model": "gemini-2.5-flash",
#                             "pipeline": "RAG",
#                             "agentic": True
#                         }
#                     )
# 
#             placeholder.markdown(answer)
# 
#     st.session_state.messages.append(("assistant", answer))
#

!python mcp_server.py

# Commented out IPython magic to ensure Python compatibility.
# %%writefile .gitignore
# *.gdoc
# *.gsheet
# *.gslides
# *.ipynb_checkpoints/
# drive/
# __pycache__/

!streamlit run app.py